Overview: Modular building blocks for the vision transformer architecture.

Usage: Imported by `vision_transformer.py` and trainers; extend or swap components to experiment with architectures.

Contents:
- attention.py: Multi-head attention and attention-related layers/utilities.
- conv.py: Convolutional layers used for patch embeddings or hybrid models.
- embeddings.py: Token/patch embedding layers and related utilities.
- mlp.py: Feed-forward/MLP blocks used within transformer layers.
- positional_encodings.py: Positional encoding implementations/utilities.
