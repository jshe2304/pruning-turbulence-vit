wandb: Currently logged in as: jshe (jshe-university-of-chicago) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /glade/u/home/jshen/pruning-turbulence-vit/wandb/run-20260205_100710-3mo3qalu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-violet-133
wandb: â­ï¸ View project at https://wandb.ai/jshe-university-of-chicago/turbulence-vit-prune
wandb: ðŸš€ View run at https://wandb.ai/jshe-university-of-chicago/turbulence-vit-prune/runs/3mo3qalu

============================================================
PRUNING ITERATION 1/11
Pruning 6.0% of remaining weights, then finetuning for 128 epochs
============================================================
Pruned 94,372 / 1,613,440 parameters (5.8% sparsity)
Epoch    0 | train_loss=2.626801e-04 | val_loss=2.981700e-04 | lr=1.01e-05
Epoch    1 | train_loss=2.645763e-04 | val_loss=3.003324e-04 | lr=2.00e-05
Epoch    2 | train_loss=2.640145e-04 | val_loss=2.994877e-04 | lr=2.00e-05
Epoch    3 | train_loss=2.642371e-04 | val_loss=2.992393e-04 | lr=1.00e-05
Epoch    4 | train_loss=2.594520e-04 | val_loss=2.947501e-04 | lr=1.00e-05
Epoch    5 | train_loss=2.604780e-04 | val_loss=2.960503e-04 | lr=1.00e-05
Epoch    6 | train_loss=2.595644e-04 | val_loss=2.945723e-04 | lr=1.00e-05
Epoch    7 | train_loss=2.593581e-04 | val_loss=2.942703e-04 | lr=1.00e-05
Epoch    8 | train_loss=2.597407e-04 | val_loss=2.948359e-04 | lr=1.00e-05
Epoch    9 | train_loss=2.592499e-04 | val_loss=2.945125e-04 | lr=1.00e-05
Epoch   10 | train_loss=2.584151e-04 | val_loss=2.927139e-04 | lr=1.00e-05
Epoch   11 | train_loss=2.580173e-04 | val_loss=2.930456e-04 | lr=1.00e-05
Epoch   12 | train_loss=2.583591e-04 | val_loss=2.934585e-04 | lr=1.00e-05
Epoch   13 | train_loss=2.581455e-04 | val_loss=2.929026e-04 | lr=5.00e-06
Epoch   14 | train_loss=2.555817e-04 | val_loss=2.905148e-04 | lr=5.00e-06
Epoch   15 | train_loss=2.556491e-04 | val_loss=2.900522e-04 | lr=5.00e-06
Epoch   16 | train_loss=2.552651e-04 | val_loss=2.896006e-04 | lr=5.00e-06
Epoch   17 | train_loss=2.554989e-04 | val_loss=2.896653e-04 | lr=5.00e-06
Epoch   18 | train_loss=2.554108e-04 | val_loss=2.899099e-04 | lr=5.00e-06
Epoch   19 | train_loss=2.550370e-04 | val_loss=2.892353e-04 | lr=5.00e-06
Epoch   20 | train_loss=2.547883e-04 | val_loss=2.892544e-04 | lr=5.00e-06
Epoch   21 | train_loss=2.547494e-04 | val_loss=2.889836e-04 | lr=5.00e-06
Epoch   22 | train_loss=2.545009e-04 | val_loss=2.891907e-04 | lr=5.00e-06
Epoch   23 | train_loss=2.544514e-04 | val_loss=2.885532e-04 | lr=5.00e-06
Epoch   24 | train_loss=2.539507e-04 | val_loss=2.887673e-04 | lr=5.00e-06
Epoch   25 | train_loss=2.541652e-04 | val_loss=2.882774e-04 | lr=5.00e-06
Epoch   26 | train_loss=2.539576e-04 | val_loss=2.883197e-04 | lr=5.00e-06
Epoch   27 | train_loss=2.537221e-04 | val_loss=2.878224e-04 | lr=5.00e-06
Epoch   28 | train_loss=2.538606e-04 | val_loss=2.884583e-04 | lr=5.00e-06
Epoch   29 | train_loss=2.531420e-04 | val_loss=2.869883e-04 | lr=5.00e-06
Epoch   30 | train_loss=2.534473e-04 | val_loss=2.877474e-04 | lr=5.00e-06
Epoch   31 | train_loss=2.532734e-04 | val_loss=2.875516e-04 | lr=5.00e-06
Epoch   32 | train_loss=2.530699e-04 | val_loss=2.874465e-04 | lr=2.50e-06
Epoch   33 | train_loss=2.521579e-04 | val_loss=2.857761e-04 | lr=2.50e-06
Epoch   34 | train_loss=2.516888e-04 | val_loss=2.856770e-04 | lr=2.50e-06
Epoch   35 | train_loss=2.517686e-04 | val_loss=2.856313e-04 | lr=2.50e-06
Epoch   36 | train_loss=2.515693e-04 | val_loss=2.853858e-04 | lr=2.50e-06
Epoch   37 | train_loss=2.518743e-04 | val_loss=2.860540e-04 | lr=2.50e-06
Epoch   38 | train_loss=2.515043e-04 | val_loss=2.856926e-04 | lr=2.50e-06
Epoch   39 | train_loss=2.514040e-04 | val_loss=2.849437e-04 | lr=2.50e-06
Epoch   40 | train_loss=2.513241e-04 | val_loss=2.853539e-04 | lr=2.50e-06
Epoch   41 | train_loss=2.513722e-04 | val_loss=2.853804e-04 | lr=2.50e-06
Epoch   42 | train_loss=2.511861e-04 | val_loss=2.853270e-04 | lr=1.25e-06
Epoch   43 | train_loss=2.506446e-04 | val_loss=2.845974e-04 | lr=1.25e-06
Epoch   44 | train_loss=2.505472e-04 | val_loss=2.845629e-04 | lr=1.25e-06
Epoch   45 | train_loss=2.504598e-04 | val_loss=2.844500e-04 | lr=1.25e-06
Epoch   46 | train_loss=2.504519e-04 | val_loss=2.841665e-04 | lr=1.25e-06
Epoch   47 | train_loss=2.503963e-04 | val_loss=2.843118e-04 | lr=1.25e-06
Epoch   48 | train_loss=2.504318e-04 | val_loss=2.845600e-04 | lr=1.25e-06
Epoch   49 | train_loss=2.504007e-04 | val_loss=2.842420e-04 | lr=6.25e-07

============================================================
PRUNING ITERATION 2/11
Pruning 6.0% of remaining weights, then finetuning for 128 epochs
============================================================
Pruned 183,082 / 1,613,440 parameters (11.3% sparsity)
Epoch    0 | train_loss=2.564334e-04 | val_loss=2.900347e-04 | lr=1.01e-05
Epoch    1 | train_loss=2.575233e-04 | val_loss=2.914606e-04 | lr=2.00e-05
Epoch    2 | train_loss=2.568982e-04 | val_loss=2.903018e-04 | lr=2.00e-05
Epoch    3 | train_loss=2.581200e-04 | val_loss=2.910540e-04 | lr=1.00e-05
Epoch    4 | train_loss=2.525791e-04 | val_loss=2.857553e-04 | lr=1.00e-05
Epoch    5 | train_loss=2.534643e-04 | val_loss=2.867207e-04 | lr=1.00e-05
Epoch    6 | train_loss=2.525896e-04 | val_loss=2.855476e-04 | lr=1.00e-05
Epoch    7 | train_loss=2.526878e-04 | val_loss=2.854950e-04 | lr=1.00e-05
Epoch    8 | train_loss=2.520799e-04 | val_loss=2.851739e-04 | lr=1.00e-05
Epoch    9 | train_loss=2.520035e-04 | val_loss=2.851503e-04 | lr=1.00e-05
Epoch   10 | train_loss=2.512331e-04 | val_loss=2.836151e-04 | lr=1.00e-05
Epoch   11 | train_loss=2.510586e-04 | val_loss=2.839907e-04 | lr=1.00e-05
Epoch   12 | train_loss=2.512672e-04 | val_loss=2.839827e-04 | lr=1.00e-05
Epoch   13 | train_loss=2.507731e-04 | val_loss=2.835582e-04 | lr=1.00e-05
Epoch   14 | train_loss=2.501610e-04 | val_loss=2.828500e-04 | lr=1.00e-05
Epoch   15 | train_loss=2.498669e-04 | val_loss=2.820056e-04 | lr=1.00e-05
Epoch   16 | train_loss=2.498754e-04 | val_loss=2.825079e-04 | lr=1.00e-05
Epoch   17 | train_loss=2.493477e-04 | val_loss=2.820591e-04 | lr=1.00e-05
Epoch   18 | train_loss=2.493842e-04 | val_loss=2.816027e-04 | lr=1.00e-05
Epoch   19 | train_loss=2.488385e-04 | val_loss=2.811858e-04 | lr=1.00e-05
Epoch   20 | train_loss=2.500904e-04 | val_loss=2.827172e-04 | lr=1.00e-05
Epoch   21 | train_loss=2.484569e-04 | val_loss=2.808782e-04 | lr=1.00e-05
Epoch   22 | train_loss=2.486230e-04 | val_loss=2.811086e-04 | lr=1.00e-05
Epoch   23 | train_loss=2.478855e-04 | val_loss=2.801198e-04 | lr=1.00e-05
Epoch   24 | train_loss=2.471592e-04 | val_loss=2.801038e-04 | lr=1.00e-05
Epoch   25 | train_loss=2.480195e-04 | val_loss=2.807960e-04 | lr=1.00e-05
Epoch   26 | train_loss=2.475482e-04 | val_loss=2.801339e-04 | lr=5.00e-06
Epoch   27 | train_loss=2.455011e-04 | val_loss=2.776525e-04 | lr=5.00e-06
Epoch   28 | train_loss=2.458163e-04 | val_loss=2.784117e-04 | lr=5.00e-06
Epoch   29 | train_loss=2.451346e-04 | val_loss=2.770179e-04 | lr=5.00e-06
Epoch   30 | train_loss=2.454574e-04 | val_loss=2.778061e-04 | lr=5.00e-06
Epoch   31 | train_loss=2.452949e-04 | val_loss=2.774434e-04 | lr=5.00e-06
Epoch   32 | train_loss=2.450455e-04 | val_loss=2.774916e-04 | lr=2.50e-06
Epoch   33 | train_loss=2.442145e-04 | val_loss=2.759643e-04 | lr=2.50e-06
Epoch   34 | train_loss=2.437290e-04 | val_loss=2.757166e-04 | lr=2.50e-06
Epoch   35 | train_loss=2.438984e-04 | val_loss=2.758461e-04 | lr=2.50e-06
Epoch   36 | train_loss=2.436594e-04 | val_loss=2.754251e-04 | lr=2.50e-06
Epoch   37 | train_loss=2.438506e-04 | val_loss=2.758637e-04 | lr=2.50e-06
Epoch   38 | train_loss=2.436141e-04 | val_loss=2.758719e-04 | lr=2.50e-06
Epoch   39 | train_loss=2.435211e-04 | val_loss=2.750688e-04 | lr=2.50e-06
Epoch   40 | train_loss=2.434811e-04 | val_loss=2.754804e-04 | lr=2.50e-06
Epoch   41 | train_loss=2.435511e-04 | val_loss=2.756277e-04 | lr=2.50e-06
Epoch   42 | train_loss=2.434374e-04 | val_loss=2.756461e-04 | lr=1.25e-06
Epoch   43 | train_loss=2.427874e-04 | val_loss=2.747579e-04 | lr=1.25e-06
Epoch   44 | train_loss=2.427110e-04 | val_loss=2.747881e-04 | lr=1.25e-06
Epoch   45 | train_loss=2.426403e-04 | val_loss=2.746735e-04 | lr=1.25e-06
Epoch   46 | train_loss=2.426198e-04 | val_loss=2.744060e-04 | lr=1.25e-06
Epoch   47 | train_loss=2.425699e-04 | val_loss=2.745108e-04 | lr=1.25e-06
Epoch   48 | train_loss=2.426263e-04 | val_loss=2.746582e-04 | lr=1.25e-06
Epoch   49 | train_loss=2.426272e-04 | val_loss=2.745724e-04 | lr=6.25e-07

============================================================
PRUNING ITERATION 3/11
Pruning 6.0% of remaining weights, then finetuning for 128 epochs
============================================================
Pruned 266,469 / 1,613,440 parameters (16.5% sparsity)
Epoch    0 | train_loss=2.526259e-04 | val_loss=2.839929e-04 | lr=1.01e-05
Epoch    1 | train_loss=2.523447e-04 | val_loss=2.839501e-04 | lr=2.00e-05
Epoch    2 | train_loss=2.509781e-04 | val_loss=2.823226e-04 | lr=2.00e-05
Epoch    3 | train_loss=2.521729e-04 | val_loss=2.834862e-04 | lr=2.00e-05
Epoch    4 | train_loss=2.499736e-04 | val_loss=2.805042e-04 | lr=2.00e-05
Epoch    5 | train_loss=2.503588e-04 | val_loss=2.813098e-04 | lr=2.00e-05
Epoch    6 | train_loss=2.485972e-04 | val_loss=2.797365e-04 | lr=2.00e-05
Epoch    7 | train_loss=2.494817e-04 | val_loss=2.814478e-04 | lr=2.00e-05
Epoch    8 | train_loss=2.471431e-04 | val_loss=2.777738e-04 | lr=2.00e-05
Epoch    9 | train_loss=2.474318e-04 | val_loss=2.777313e-04 | lr=2.00e-05
Epoch   10 | train_loss=2.471887e-04 | val_loss=2.784072e-04 | lr=2.00e-05
Epoch   11 | train_loss=2.468677e-04 | val_loss=2.784533e-04 | lr=2.00e-05
Epoch   12 | train_loss=2.464744e-04 | val_loss=2.773149e-04 | lr=2.00e-05
Epoch   13 | train_loss=2.459431e-04 | val_loss=2.765298e-04 | lr=2.00e-05
Epoch   14 | train_loss=2.448501e-04 | val_loss=2.759196e-04 | lr=2.00e-05
Epoch   15 | train_loss=2.439166e-04 | val_loss=2.754386e-04 | lr=2.00e-05
Epoch   16 | train_loss=2.446256e-04 | val_loss=2.754205e-04 | lr=2.00e-05
Epoch   17 | train_loss=2.456377e-04 | val_loss=2.770603e-04 | lr=2.00e-05
Epoch   18 | train_loss=2.433477e-04 | val_loss=2.737405e-04 | lr=2.00e-05
Epoch   19 | train_loss=2.438915e-04 | val_loss=2.739951e-04 | lr=2.00e-05
Epoch   20 | train_loss=2.444030e-04 | val_loss=2.743605e-04 | lr=2.00e-05
Epoch   21 | train_loss=2.432114e-04 | val_loss=2.743524e-04 | lr=1.00e-05
Epoch   22 | train_loss=2.397866e-04 | val_loss=2.701691e-04 | lr=1.00e-05
Epoch   23 | train_loss=2.391159e-04 | val_loss=2.694273e-04 | lr=1.00e-05
Epoch   24 | train_loss=2.386799e-04 | val_loss=2.694065e-04 | lr=1.00e-05
Epoch   25 | train_loss=2.395397e-04 | val_loss=2.702907e-04 | lr=1.00e-05
Epoch   26 | train_loss=2.391259e-04 | val_loss=2.696741e-04 | lr=5.00e-06
Epoch   27 | train_loss=2.371565e-04 | val_loss=2.672161e-04 | lr=5.00e-06
Epoch   28 | train_loss=2.370547e-04 | val_loss=2.676923e-04 | lr=5.00e-06
Epoch   29 | train_loss=2.367021e-04 | val_loss=2.666666e-04 | lr=5.00e-06
Epoch   30 | train_loss=2.369380e-04 | val_loss=2.672166e-04 | lr=5.00e-06
Epoch   31 | train_loss=2.368143e-04 | val_loss=2.668832e-04 | lr=5.00e-06
Epoch   32 | train_loss=2.367504e-04 | val_loss=2.671138e-04 | lr=2.50e-06
Epoch   33 | train_loss=2.358502e-04 | val_loss=2.657092e-04 | lr=2.50e-06
Epoch   34 | train_loss=2.353828e-04 | val_loss=2.654968e-04 | lr=2.50e-06
Epoch   35 | train_loss=2.355858e-04 | val_loss=2.655892e-04 | lr=2.50e-06
Epoch   36 | train_loss=2.353836e-04 | val_loss=2.652724e-04 | lr=2.50e-06
Epoch   37 | train_loss=2.355001e-04 | val_loss=2.656040e-04 | lr=2.50e-06
Epoch   38 | train_loss=2.353496e-04 | val_loss=2.656407e-04 | lr=2.50e-06
Epoch   39 | train_loss=2.352493e-04 | val_loss=2.649258e-04 | lr=2.50e-06
Epoch   40 | train_loss=2.352785e-04 | val_loss=2.653134e-04 | lr=2.50e-06
Epoch   41 | train_loss=2.353178e-04 | val_loss=2.652833e-04 | lr=2.50e-06
Epoch   42 | train_loss=2.351845e-04 | val_loss=2.653648e-04 | lr=1.25e-06
Epoch   43 | train_loss=2.345894e-04 | val_loss=2.646411e-04 | lr=1.25e-06
Epoch   44 | train_loss=2.344844e-04 | val_loss=2.644681e-04 | lr=1.25e-06
Epoch   45 | train_loss=2.344375e-04 | val_loss=2.645108e-04 | lr=1.25e-06
Epoch   46 | train_loss=2.344388e-04 | val_loss=2.643218e-04 | lr=1.25e-06
Epoch   47 | train_loss=2.343784e-04 | val_loss=2.643443e-04 | lr=1.25e-06
Epoch   48 | train_loss=2.344353e-04 | val_loss=2.644525e-04 | lr=1.25e-06
Epoch   49 | train_loss=2.344248e-04 | val_loss=2.644826e-04 | lr=6.25e-07

============================================================
PRUNING ITERATION 4/11
Pruning 6.0% of remaining weights, then finetuning for 128 epochs
============================================================
Pruned 344,853 / 1,613,440 parameters (21.4% sparsity)
Epoch    0 | train_loss=2.504894e-04 | val_loss=2.805608e-04 | lr=1.01e-05
Epoch    1 | train_loss=2.465582e-04 | val_loss=2.765920e-04 | lr=2.00e-05
Epoch    2 | train_loss=2.448970e-04 | val_loss=2.744603e-04 | lr=2.00e-05
Epoch    3 | train_loss=2.450508e-04 | val_loss=2.741730e-04 | lr=2.00e-05
Epoch    4 | train_loss=2.433022e-04 | val_loss=2.724876e-04 | lr=2.00e-05
Epoch    5 | train_loss=2.429540e-04 | val_loss=2.717990e-04 | lr=2.00e-05
Epoch    6 | train_loss=2.421008e-04 | val_loss=2.716522e-04 | lr=2.00e-05
Epoch    7 | train_loss=2.424944e-04 | val_loss=2.725069e-04 | lr=2.00e-05
Epoch    8 | train_loss=2.404403e-04 | val_loss=2.690623e-04 | lr=2.00e-05
Epoch    9 | train_loss=2.406940e-04 | val_loss=2.686792e-04 | lr=2.00e-05
Epoch   10 | train_loss=2.397321e-04 | val_loss=2.692699e-04 | lr=2.00e-05
Epoch   11 | train_loss=2.397553e-04 | val_loss=2.689682e-04 | lr=2.00e-05
Epoch   12 | train_loss=2.390196e-04 | val_loss=2.673998e-04 | lr=2.00e-05
Epoch   13 | train_loss=2.387989e-04 | val_loss=2.672903e-04 | lr=2.00e-05
Epoch   14 | train_loss=2.380999e-04 | val_loss=2.667934e-04 | lr=2.00e-05
Epoch   15 | train_loss=2.363572e-04 | val_loss=2.654138e-04 | lr=2.00e-05
Epoch   16 | train_loss=2.368263e-04 | val_loss=2.654902e-04 | lr=2.00e-05
Epoch   17 | train_loss=2.376712e-04 | val_loss=2.670089e-04 | lr=2.00e-05
Epoch   18 | train_loss=2.361453e-04 | val_loss=2.649391e-04 | lr=2.00e-05
Epoch   19 | train_loss=2.362383e-04 | val_loss=2.647331e-04 | lr=2.00e-05
Epoch   20 | train_loss=2.370933e-04 | val_loss=2.656970e-04 | lr=2.00e-05
Epoch   21 | train_loss=2.359669e-04 | val_loss=2.651522e-04 | lr=2.00e-05
Epoch   22 | train_loss=2.389436e-04 | val_loss=2.671207e-04 | lr=1.00e-05
Epoch   23 | train_loss=2.318277e-04 | val_loss=2.604622e-04 | lr=1.00e-05
Epoch   24 | train_loss=2.318176e-04 | val_loss=2.611540e-04 | lr=1.00e-05
Epoch   25 | train_loss=2.328075e-04 | val_loss=2.619860e-04 | lr=1.00e-05
Epoch   26 | train_loss=2.321923e-04 | val_loss=2.607985e-04 | lr=5.00e-06
Epoch   27 | train_loss=2.301843e-04 | val_loss=2.583282e-04 | lr=5.00e-06
Epoch   28 | train_loss=2.301139e-04 | val_loss=2.590312e-04 | lr=5.00e-06
Epoch   29 | train_loss=2.297517e-04 | val_loss=2.581570e-04 | lr=5.00e-06
Epoch   30 | train_loss=2.301402e-04 | val_loss=2.585460e-04 | lr=5.00e-06
Epoch   31 | train_loss=2.298743e-04 | val_loss=2.583091e-04 | lr=5.00e-06
Epoch   32 | train_loss=2.298700e-04 | val_loss=2.585608e-04 | lr=2.50e-06
Epoch   33 | train_loss=2.289482e-04 | val_loss=2.569771e-04 | lr=2.50e-06
Epoch   34 | train_loss=2.285502e-04 | val_loss=2.568954e-04 | lr=2.50e-06
Epoch   35 | train_loss=2.286994e-04 | val_loss=2.568445e-04 | lr=2.50e-06
Epoch   36 | train_loss=2.285105e-04 | val_loss=2.565676e-04 | lr=2.50e-06
Epoch   37 | train_loss=2.286654e-04 | val_loss=2.568900e-04 | lr=2.50e-06
Epoch   38 | train_loss=2.285289e-04 | val_loss=2.571188e-04 | lr=2.50e-06
Epoch   39 | train_loss=2.283343e-04 | val_loss=2.561379e-04 | lr=2.50e-06
Epoch   40 | train_loss=2.284405e-04 | val_loss=2.568051e-04 | lr=2.50e-06
Epoch   41 | train_loss=2.284771e-04 | val_loss=2.567320e-04 | lr=2.50e-06
Epoch   42 | train_loss=2.283169e-04 | val_loss=2.567034e-04 | lr=1.25e-06
=>> PBS: job killed: walltime 43220 exceeded limit 43200
W0205 22:06:30.530000 53903 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received Signals.SIGTERM death signal, shutting down workers
W0205 22:06:30.541000 53903 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 57832 closing signal SIGTERM
W0205 22:06:30.543000 53903 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 57834 closing signal SIGTERM
W0205 22:06:30.545000 53903 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 57836 closing signal SIGTERM
W0205 22:06:30.545000 53903 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 57838 closing signal SIGTERM
Traceback (most recent call last):
  File "/glade/work/jshen/conda-envs/vit/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/glade/work/jshen/conda-envs/vit/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/glade/work/jshen/conda-envs/vit/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/glade/work/jshen/conda-envs/vit/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/glade/work/jshen/conda-envs/vit/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/glade/work/jshen/conda-envs/vit/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 261, in launch_agent
    result = agent.run()
  File "/glade/work/jshen/conda-envs/vit/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
  File "/glade/work/jshen/conda-envs/vit/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 711, in run
    result = self._invoke_run(role)
  File "/glade/work/jshen/conda-envs/vit/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 870, in _invoke_run
    time.sleep(monitor_interval)
  File "/glade/work/jshen/conda-envs/vit/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 53903 got signal: 15
